{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f463d8-7825-43ba-8a60-b512a4f82e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "models:\n",
    "  - model: flammenai/Mahou-1.3-llama3-8B\n",
    "    parameters:\n",
    "      weight: 1.0\n",
    "  - model: Danielbrdz/Barcenas-Llama3-8b-ORPO\n",
    "    parameters:\n",
    "      weight: 1.0\n",
    "  - model: Weyaxi/Einstein-v6.1-Llama3-8B\n",
    "    parameters:\n",
    "      weight: 1.0\n",
    "merge_method: linear\n",
    "tokenizer_source: union\n",
    "dtype: float16\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a35b836-5662-45c2-99f2-7b7abb7a261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml','w',encoding='utf-8') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a742ebed-3d68-4dc2-8758-4b190f2e4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mergekit' already exists and is not an empty directory.\n",
      "/Users/naveenpoliasetty/Merging /mergekit\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/arcee-ai/mergekit.git\n",
    "%cd mergekit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b2380c-878e-431e-a450-387fabdceca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"models/llama-V1/merged/mini\"\n",
    "LORA_MERGE_CACHE = \"/tmp\"\n",
    "CONFIG_YAML = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e414a-b2e5-4cf2-b94b-8447f6b6ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "\n",
    "with open('config.yaml','r',encoding='utf-8')as f:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(f))\n",
    "run_merge (\n",
    "    merge_config,\n",
    "    out_path = OUTPUT_PATH,\n",
    "    options = MergeOptions(\n",
    "        lora_merge_cache = LORA_MERGE_CACHE,\n",
    "        copy_tokenizer = True,\n",
    "        lazy_unpickle = False,\n",
    "        low_cpu_memory = False),\n",
    "        )\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519b01-0477-453a-9b5d-ff0403914a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "QUANT_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/llama-V1/merged/mini\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/llama-V1/merged/mini\", quantization_config=QUANT_CONFIG)\n",
    "\n",
    "user_message = \"Write a recursive function that calculates Fibonacci sequence in Python.\"\n",
    "prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{user_message}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs.to(\"cuda\"),\n",
    "                              max_new_tokens=512,\n",
    "                              num_beams=10,\n",
    "                              early_stopping=True,\n",
    "                              no_repeat_ngram_size=2,\n",
    "                              )\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result[0])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1c63a-af86-48da-a5ef-b7048b5a2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ff7d4-7956-4eb9-a7f7-9d14727e4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Naveenpoliasetty/llama3-8B-merged-V-small\")\n",
    "tokenizer.push_to_hub(\"Naveenpoliasetty/llama3-8B-merged-V-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d78f61-1f12-4cca-a5dd-866a14271385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
